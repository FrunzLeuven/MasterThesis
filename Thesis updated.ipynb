{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "694b7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutup; shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "51d46f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import fpgrowth, apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from spmf import Spmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "01469f2d-fe5c-46cf-ba6c-7cf265e3379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(df):\n",
    "    df = df.dropna()\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "    df['hadm_id'] = df['hadm_id'].astype(int)\n",
    "    # combine admission with subject_ids\n",
    "    df['subject_id'] = df['subject_id'].astype(\"str\")+\"_\"+df['hadm_id'].astype(\"str\")\n",
    "    df = df.drop_duplicates().drop(columns=[\"hadm_id\"])\n",
    "    return df\n",
    "    \n",
    "def map_mimic_to_loinc(df):\n",
    "    lonic = pd.read_csv(\"./mimic_to_loinc.csv\") \n",
    "    joined_df = pd.merge(df.query(\"tbl_name=='labevents'\"), lonic, how='inner', left_on='col_id', right_on='itemid')\n",
    "    lab_events = joined_df[['subject_id','charttime', 'loinc_code', 'tbl_name']].dropna().drop_duplicates()\n",
    "    lab_events = lab_events.rename(columns={\"loinc_code\":\"col_id\"})\n",
    "    df = pd.concat([df.query(\"tbl_name!='labevents'\"), lab_events]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Generate unique 5-length digit IDs for each string\n",
    "def generate_string_pharm(df):\n",
    "    unique_strings = df.query(\"tbl_name=='pharmacy'\")['col_id'].drop_duplicates().tolist()\n",
    "    id_mapping = {string: f\"{index:05}\" for index, string in enumerate(unique_strings)}\n",
    "    pharm_mapping = pd.DataFrame(list(id_mapping.items()), columns=['col_id', 'new_col_id'])\n",
    "    return pharm_mapping\n",
    "\n",
    "def map_pharmacy_codes(df, pharm_mapping):\n",
    "    df = pd.merge(df, pharm_mapping, how='left', left_on='col_id', right_on='col_id')\n",
    "    df['col_id'] = (np.where(df['tbl_name'] == 'pharmacy', df['new_col_id'], df['col_id']))\n",
    "    df = df.drop(columns=[\"new_col_id\"])\n",
    "    return df\n",
    "\n",
    "def add_prefix(df):\n",
    "    # create a prefix based on 3 characters of each tbl_name\n",
    "    df['prefix'] = df['tbl_name'].apply(lambda x: x[0:3]+'_')\n",
    "    # add the prefix to the col_id\n",
    "    df['col_id'] = df['prefix']+df['col_id']\n",
    "    df = df.drop(columns=[\"prefix\"])\n",
    "    return df\n",
    "\n",
    "def group_into_list(df):\n",
    "    df = df.sort_values(['subject_id','charttime'], ascending=[True, True])\n",
    "    # add all items to a list \n",
    "    df = (df.groupby(['subject_id', 'charttime'])['col_id'].apply(list)\n",
    "            .reset_index(name='col_id_list'))\n",
    "    return df\n",
    "\n",
    "def closed_patterns(frequent):\n",
    "    su = frequent.support.unique()#all unique support count\n",
    "    #Dictionay storing itemset with same support count key\n",
    "    fredic = {}\n",
    "    for i in range(len(su)):\n",
    "        inset = list(frequent.loc[frequent.support ==su[i]]['itemsets'])\n",
    "        fredic[su[i]] = inset\n",
    "    #Dictionay storing itemset with  support count <= key\n",
    "    fredic2 = {}\n",
    "    for i in range(len(su)):\n",
    "        inset2 = list(frequent.loc[frequent.support<=su[i]]['itemsets'])\n",
    "        fredic2[su[i]] = inset2\n",
    "    \n",
    "    #Find Closed frequent itemset\n",
    "    cl = []\n",
    "    for index, row in frequent.iterrows():\n",
    "        isclose = True\n",
    "        cli = row['itemsets']\n",
    "        cls = row['support']\n",
    "        checkset = fredic[cls]\n",
    "        for i in checkset:\n",
    "            if (cli!=i):\n",
    "                if(frozenset.issubset(cli,i)):\n",
    "                    isclose = False\n",
    "                    break\n",
    "        \n",
    "        if(isclose):\n",
    "            cl.append(row['itemsets'])\n",
    "    return cl\n",
    "    \n",
    "def generate_frequent_itemsets(df, min_support):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(df[ \"col_id_list\"].to_list()).transform(df[ \"col_id_list\"].to_list())\n",
    "    onehot = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    patterns = fpgrowth(onehot, min_support=min_support, use_colnames=True)\n",
    "    patterns['itemsets'] = patterns['itemsets'].apply(lambda x: frozenset(x))\n",
    "    cl = closed_patterns(patterns)\n",
    "    patterns = patterns[patterns['itemsets'].isin(cl)]\n",
    "    return patterns\n",
    "\n",
    "def get_buckets(patterns):\n",
    "    patterns['length'] = patterns['itemsets'].apply(len)\n",
    "    patterns = patterns.sort_values([\"length\", \"support\"],ascending=[False, False])\n",
    "    \n",
    "    buckets = (patterns\n",
    "               .groupby(['length'])['itemsets'].apply(list).reset_index(name='buckets')\n",
    "              ).sort_values(\"length\",ascending=False)['buckets'].to_list()       \n",
    "    \n",
    "    return buckets\n",
    "\n",
    "def basket_mappings(patterns):\n",
    "    long_baskets = patterns.query(\"length>1\")['itemsets'].drop_duplicates().to_list()\n",
    "    id_mapping_basket = {string: frozenset({f\"basket_{index:05}\"}) for index, string in enumerate(long_baskets)}\n",
    "    mapping_basket_df = pd.DataFrame(list(id_mapping_basket.items()), columns=['basket', 'basket_id'])\n",
    "    return id_mapping_basket, mapping_basket_df\n",
    "\n",
    "def break_down(pattern, buckets, id_mapping_basket):\n",
    "    best_subsets = []\n",
    "    output = pattern \n",
    "    for B in buckets:\n",
    "        if len(B[0])<len(pattern):\n",
    "            for SE in B:\n",
    "                if SE.issubset(pattern):\n",
    "                    best_subsets.append(SE)\n",
    "                    pattern = pattern.difference(SE)\n",
    "                    if not pattern:\n",
    "                         break\n",
    "    if len(best_subsets)>0:\n",
    "        best_subsets = [id_mapping_basket[best_subset] if len(best_subset) > 1 else best_subset for best_subset in best_subsets]\n",
    "        if pattern:\n",
    "            best_subsets.append(pattern)\n",
    "        output = frozenset.union(*best_subsets)\n",
    "    return list(output)\n",
    "    \n",
    "def get_final_mapping(df):\n",
    "    df = df.sort_values(['subject_id','charttime'], ascending=[True, True])\n",
    "    df['col_id_list'] = df['col_id_list'].apply(list)\n",
    "    all_items_mapping = df.explode('col_id_list')['col_id_list'].drop_duplicates().reset_index(drop=True).to_dict()\n",
    "    reverse_final_mapping = df.explode('col_id_list')['col_id_list'].drop_duplicates().reset_index(drop=True).to_dict()\n",
    "    final_mapping = {v: k for k, v in reverse_final_mapping.items()}\n",
    "    return final_mapping, reverse_final_mapping\n",
    "\n",
    "def apply_mapping(item_list, final_mapping):\n",
    "    return [final_mapping[item] for item in item_list]\n",
    "\n",
    "def get_sequences(df):\n",
    "    sequences = df.groupby(['subject_id'])['col_id_list'].apply(list).reset_index(name='sequences')['sequences'].to_list()\n",
    "    return sequences\n",
    "\n",
    "def convert_string_list(string_list):\n",
    "    return [list(map(int, s.split())) if ' ' in s else [int(s)] for s in string_list]\n",
    "\n",
    "def reverse_apply_final_mapping(pattern):\n",
    "    mapped_pattern = []\n",
    "    for sublist in pattern:\n",
    "        mapped_sublist = [reverse_final_mapping[num] for num in sublist]\n",
    "        mapped_pattern.append(mapped_sublist)\n",
    "    return mapped_pattern\n",
    "def convert_to_frozenset(loinc_list):\n",
    "    return frozenset(loinc_list.split(', '))\n",
    "\n",
    "def convert_loinc_to_panel(df):\n",
    "    loinc_panels = pd.read_csv(\"./mimic_loinc_panels.csv\")\n",
    "    loinc_panels['LoincList'] = loinc_panels['LoincList'].apply(convert_to_frozenset)\n",
    "    loinc_panels['ParentLoinc'] = loinc_panels['ParentLoinc'].apply(lambda x: frozenset([x]))\n",
    "    loinc_panels['length'] = loinc_panels['LoincList'].apply(len)\n",
    "    loinc_dict = dict(zip(loinc_panels['LoincList'], loinc_panels['ParentLoinc']))\n",
    "    buckets_loinc = (loinc_panels.groupby(['length'])['LoincList']\n",
    "                                 .apply(list).reset_index(name='buckets')\n",
    "                                 .sort_values(\"length\",ascending=False)['buckets'].to_list()) \n",
    "    \n",
    "    lab_panels = group_into_list(df.query(\"tbl_name=='labevents'\"))\n",
    "    lab_panels['col_id_list'] = lab_panels[\"col_id_list\"].apply(lambda x: break_down(frozenset(x), buckets_loinc, loinc_dict))\n",
    "    \n",
    "    lab_panels = lab_panels.explode('col_id_list').rename(columns={'col_id_list': 'col_id'})\n",
    "    lab_panels['tbl_name'] = 'labevents'\n",
    "    df = pd.concat([df.query(\"tbl_name!='labevents'\"), lab_panels]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9d529f29-b883-4ba5-ba1e-77d33ffb8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonic = pd.read_csv(\"./mimic_to_loinc.csv\") \n",
    "loinc_panels = pd.read_csv(\"./mimic_loinc_panels.csv\")\n",
    "path = \"./mimic_raw_data2.csv\"\n",
    "df = pd.read_csv(path).drop(columns=\"col_value\") \n",
    "df = basic_preprocessing(df)\n",
    "df = map_mimic_to_loinc(df)\n",
    "df = convert_loinc_to_panel(df)\n",
    "pharm_mapping = generate_string_pharm(df)\n",
    "df = map_pharmacy_codes(df, pharm_mapping)\n",
    "df = add_prefix(df)\n",
    "df = group_into_list(df)\n",
    "patterns = generate_frequent_itemsets(df, 0.08)\n",
    "patterns['length'] = patterns['itemsets'].apply(len)\n",
    "id_mapping_basket, mapping_basket_df = basket_mappings(patterns)\n",
    "buckets = get_buckets(patterns)\n",
    "df['col_id_list'] = df[\"col_id_list\"].apply(lambda x: break_down(frozenset(x), buckets, id_mapping_basket))\n",
    "final_mapping, reverse_final_mapping = get_final_mapping(df)\n",
    "df['col_id_list'] = df['col_id_list'].apply(lambda x: apply_mapping(x, final_mapping))\n",
    "sequences = get_sequences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "524e4667-c882-4ce2-a795-3ba336fcdb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Before removing NonClosed patterns there are 7330 patterns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/Users/ehambardzumy/Desktop/other_stuff/spmf.jar\n",
      "=============  Algorithm - STATISTICS =============\n",
      " Total time ~ 7074 ms\n",
      " Frequent sequences count : 5257\n",
      " Max memory (mb):158.867919921875\n",
      "Content at file spmf-output.txt\n",
      "\n",
      "===================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spmf = Spmf(\"CloSpan\", input_direct=sequences,\n",
    "             arguments=[0.08,\"\", True])\n",
    "spmf.run()\n",
    "# print(spmf.parse_output())\n",
    "df_spmf = spmf.to_pandas_dataframe(pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9da17352-0b1f-49dd-a8da-10d81cd35fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spmf['pattern'] = df_spmf['pattern'].apply(convert_string_list)\n",
    "df_spmf['pattern'] = df_spmf['pattern'].apply(reverse_apply_final_mapping)\n",
    "df_spmf['len'] = df_spmf['pattern'].apply(len)\n",
    "out = df_spmf.query(\"len>1\").sort_values(\"sup\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "fcaf6f0d-17d7-4af4-8bc1-496eccbb7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#out.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "45a2ef4a-b625-4144-a0ad-ff64906cd196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping_basket_df[mapping_basket_df[\"basket_id\"] == frozenset({\"basket_00083\"})]['basket'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2f939a6d-4391-4f24-ace2-2edb854626dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loinc_panels.query(\"ParentLoinc=='69739-1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "bab6bb78-8e1e-4eea-a919-4981923c468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lonic[lonic['loinc_code']=='14979-9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f01afc68-b470-4b3e-8d9d-12579cd521c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pharm_mapping.query(\"new_col_id == '00004'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451cb15-ffd8-47f1-8b18-01bf2e1a5b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
